{
  "personal_profile": {
    "name": "Archana M",
    "total_it_experience": "8+ years (since Dec 2004)",
    "primary_roles": [
      "Cloud Support Engineer (AWS)",
      "Oracle Application Consultant",
      "Data-Focused IT Professional"
    ],
    "career_summary": "Data-focused IT professional with 8+ years of experience across enterprise applications, cloud database support, and analytics-driven roles. Strong background in SQL, Oracle and PostgreSQL databases, data migration, reporting, and stakeholder engagement. Hands-on experience supporting AWS RDS and Aurora environments, resolving data quality and performance issues, and translating business requirements into technical solutions. Currently undergoing AI Data Analyst internship with AusBiz Consulting (8-9 months) working on intensive project work including AI-powered data pipelines and digital twin development. Certified in CompTIA Data+ and Microsoft Azure Data Fundamentals.",
    "location_preference": ["Sydney, Australia"],
    "current_status": "Currently unemployed - Actively seeking opportunities",
    "job_target": "Data Analyst / Support Engineer / ERP Consultant",
    "job_fit": {
      "location": "Currently based in Sydney - available for hybrid/on-site roles",
      "salary_expectation": "$80,000-$95,000 AUD",
      "notice_period": "Immediately available",
      "visa_status": "Australian resident"
    }
  },

  "education": [
    {
      "degree": "Bachelor of Information Technology",
      "institution": "Madras University",
      "location": "Chennai, Tamil Nadu, India",
      "year": 2004
    },
    {
      "degree": "Post Graduate Diploma in Human Resources Management",
      "institution": "Welingkar Institute",
      "location": "Mumbai, India",
      "year": 2013
    },
    {
      "degree": "Certificate IV in Accounting and Bookkeeping",
      "institution": "Monarch Institute",
      "location": "Sydney, Australia",
      "year": 2021
    }
  ],

  "certifications": [
    {
      "name": "CompTIA Data+",
      "location": "Online",
      "year": 2025
    },
    {
      "name": "Microsoft Certified: Azure Data Fundamentals",
      "location": "Online",
      "year": 2025
    },
    {
      "name": "AWS Certified Cloud Practitioner",
      "location": "Online",
      "year": 2021
    }
  ],

  "professional_experience": [
    {
      "company": "Amazon Web Services (AWS)",
      "location": "Sydney, Australia",
      "role": "Cloud Support Engineer (Oracle, PostgreSQL)",
      "duration": "Aug 2022 – Mar 2025",
      "years": 2.7,
      "quantified_impact": [
        "Supported enterprise customers at scale with 500+ customer issues resolved and 85% first-call-resolution rate",
        "Specialized in AWS RDS (Oracle and PostgreSQL) and Aurora database support for production environments",
        "Resolved RDS upgrade, configuration, and performance issues by identifying deprecated instance classes and providing CLI-based workarounds",
        "Guided customers through CloudWatch metrics analysis (CPU, memory, IOPS, latency), improving customer self-sufficiency in performance monitoring",
        "Reproduced and isolated customer-reported issues in controlled lab environments, reducing misdiagnosis risk and accelerating root cause identification",
        "Handled high-severity incidents requiring frequent customer updates and internal escalations, maintaining service trust during critical outages",
        "Conducted multiple screen-sharing sessions to help customers understand RDS performance metrics and error causes"
      ],
      "key_achievements": {
        "database_expertise": "Deep knowledge of Oracle and PostgreSQL database administration, troubleshooting, and performance optimization on AWS",
        "customer_success": "85% first-call-resolution rate through methodical troubleshooting and clear communication",
        "cross_team_collaboration": "Raised internal bugs and feature requests based on repeated customer pain points, influencing product improvements"
      }
    },
    {
      "company": "Seertree Global Services",
      "location": "Chennai, India",
      "role": "Oracle Application Consultant – Technical",
      "duration": "Dec 2014 – Mar 2015",
      "years": 0.25,
      "quantified_impact": [
        "Delivered customer reporting solutions by converting ambiguous business requirements into technically feasible designs",
        "Managed Oracle E-Business Suite (HRMS, Payroll, Finance, SCM) technical implementations and configurations"
      ]
    },
    {
      "company": "Radiare Software Solutions Ltd",
      "location": "Chennai, India",
      "role": "Associate Consultant",
      "duration": "Mar 2014 – Dec 2014",
      "years": 0.75,
      "quantified_impact": [
        "Provided functional and technical consulting on enterprise applications"
      ]
    },
    {
      "company": "IBM India Pvt Ltd",
      "location": "Bangalore, India",
      "role": "Application Programmer Consultant",
      "duration": "Nov 2007 – Jan 2009",
      "years": 1.25,
      "quantified_impact": [
        "Developed and optimized SQL and PL/SQL queries, procedures, and reports for enterprise systems",
        "Supported technical implementations and ensured operational continuity",
        "Understood business requirements and translated them into data-driven solutions"
      ],
      "metrics_examples": {
        "technical_support": "Resolved customer issues related to query logic and database behavior using SQL expertise"
      }
    },
    {
      "company": "Tata Consultancy Services Pvt Ltd",
      "location": "Mumbai, India",
      "role": "Assistant System Engineer",
      "duration": "Dec 2004 – Nov 2007",
      "years": 3,
      "quantified_impact": [
        "Started career in enterprise application support and database administration",
        "Supported Oracle Applications implementation and customization",
        "Gained foundational experience in troubleshooting and customer engagement"
      ]
    }
  ],

  "career_breaks": [
    {
      "period": "2009 – 2014",
      "duration": "5 years",
      "reason": "Family care responsibilities"
    },
    {
      "period": "2015 – 2021",
      "duration": "6 years",
      "reason": "Family care responsibilities"
    }
  ],

  "volunteer_experience": [
    {
      "organization": "Family Business",
      "location": "Chennai, Tamil Nadu, India",
      "role": "IT & Administrative Support",
      "responsibilities": [
        "Basic website management and maintenance",
        "IT infrastructure support",
        "Administrative coordination"
      ]
    }
  ],

  "recent_learning_projects": [
    {
      "project_name": "AI-Powered Food Knowledge Assistant (Food RAG)",
      "period": "Dec 2025 – Jan 2026",
      "organization": "AusBiz Consulting (Internship)",
      "location": "Sydney, Australia (Remote)",
      "role": "AI Data Analyst Intern",
      "description": "Built a data retrieval and analysis pipeline to generate structured insights from curated datasets",
      "key_accomplishments": [
        "Designed and implemented data retrieval and analysis pipeline for knowledge extraction",
        "Implemented data validation and context-aware retrieval to improve result accuracy and relevance",
        "Deployed lightweight web interface to support interactive exploration of data-driven outputs",
        "Applied RAG (Retrieval-Augmented Generation) principles to ensure accuracy and relevance of generated insights",
        "Demonstrated ability to work with modern AI/ML architectures and data engineering practices"
      ],
      "skills_demonstrated": ["Data pipeline development", "Data validation", "Web interface design", "RAG implementation", "Python scripting"]
    },
    {
      "project_name": "Building My Digital Twin",
      "period": "Jan 2026 – present (ongoing)",
      "organization": "AusBiz Consulting (Internship)",
      "location": "Sydney, Australia (Remote)",
      "role": "AI Data Analyst Intern",
      "description": "Comprehensive professional profile development with RAG-powered MCP server for interview preparation and career assessment",
      "key_accomplishments": [
        "Designed and implemented a digital twin system to capture comprehensive professional profile",
        "Built RAG-powered interview simulation system using MCP (Model Context Protocol)",
        "Integrated vector database (Upstash) for semantic search of professional capabilities",
        "Created structured interview framework that generates role-specific assessments",
        "Implemented competency scoring system based on actual profile data",
        "Tested across multiple job roles and documented assessment results",
        "Demonstrated proficiency in full-stack development including backend architecture, vector databases, and LLM integration"
      ],
      "technical_stack": ["Next.js 15.5", "TypeScript", "Upstash Vector DB", "MCP Protocol", "Groq API", "Python"],
      "skills_demonstrated": ["System design", "Full-stack development", "Vector database integration", "RAG implementation", "LLM orchestration", "Documentation"]
    }
  ],

  "skills": {
    "power_bi": {
      "proficiency": "Basic (self-learning only, no professional hands-on experience)",
      "dashboards_built": "Basic practice dashboards as part of self-learning project work",
      "key_capabilities": [
        "Basic understanding of data visualization concepts",
        "Introductory knowledge of Power Query for data transformation",
        "Familiarity with basic dashboard layout and design",
        "Conceptual understanding of data modeling and relationships"
      ],
      "note": "Limited practical experience - built simple dashboards during self-study to understand core concepts. No production or professional experience."
    },
    "excel": {
      "proficiency": "Advanced",
      "key_skills": [
        "PivotTables for summarization",
        "Power Query for ETL and transformation",
        "Advanced formulas (INDEX/MATCH, nested IFs)",
        "Data validation and structured reporting",
        "Multi-sheet analytical workbooks"
      ]
    },
    "databases": [
      "SQL - data analysis and reporting",
      "SQL Server - working knowledge",
      "Oracle Applications - 5+ years scheduled report development",
      "PL/SQL - query development"
    ],
    "office_tools": {
      "power_query": "Advanced",
      "vba": "Basic exposure",
      "access": "Basic familiarity"
    },
    "data_quality": [
      "Data cleaning and validation",
      "Source-to-report reconciliation",
      "Anomaly detection and duplicate identification",
      "Metric standardization and documentation",
      "Data governance and accuracy ownership"
    ],
    "cloud": [
      "AWS RDS",
      "AWS CloudWatch",
      "AWS CLI",
      "Incident escalation",
      "Root cause analysis"
    ],
    "programming_and_scripting": {
      "python": {
        "proficiency": "Intermediate",
        "experience_areas": [
          "Data retrieval and analysis pipelines",
          "Data validation and transformation",
          "Web framework integration (API usage)",
          "RAG (Retrieval-Augmented Generation) system development",
          "Working with modern AI/ML libraries and tools",
          "Web scraping and structured data extraction"
        ],
        "projects": [
          "Food RAG (AI-powered food knowledge assistant)",
          "Digital Twin MCP Server (vector database integration, semantic search)",
          "Data processing scripts for vector embedding"
        ],
        "note": "Internship projects completed under the guidance of AusBiz Consulting as part of AI Data Analyst internship program. Practical proficiency in application development and scripting developed through structured project work."
      },
      "shell_scripting": {
        "proficiency": "Intermediate",
        "experience_areas": [
          "AWS CLI automation and infrastructure scripting",
          "System operations and maintenance tasks",
          "Pipeline automation and data processing workflows",
          "Environment configuration and deployment scripts"
        ]
      }
    },
    "development_tools": [
      "GitHub - version control, collaborative development",
      "VS Code - primary development environment",
      "Git - branching, merging, collaborative workflows",
      "Next.js - full-stack TypeScript web framework",
      "TypeScript - strongly-typed JavaScript development",
      "REST APIs - design and consumption",
      "MCP (Model Context Protocol) - server architecture and implementation",
      "Docker basics - containerization concepts"
    ],
    "ai_ml_and_modern_tools": {
      "vector_databases": [
        "Upstash Vector - semantic search in production systems",
        "Vector embeddings and similarity search concepts"
      ],
      "llm_integration": [
        "LLM orchestration and API integration",
        "Groq API for fast inference",
        "Prompt engineering and instruction design",
        "RAG (Retrieval-Augmented Generation) system design"
      ],
      "ai_frameworks": [
        "Understanding of modern AI/ML architectures",
        "Integration of AI models into applications",
        "Data-driven decision making with AI insights"
      ]
    },
    "erp_and_business_systems": [
      "Oracle Applications (5+ years experience) - HRMS, Payroll, Finance, SCM modules",
      "Xero accounting software - bookkeeping and financial management",
      "Enterprise workflow design and optimization",
      "Business process automation"
    ],
    "soft_skills": [
      "Stakeholder communication in WAR room during critical production issues",
      "Data storytelling and business impact communication",
      "Customer education",
      "Analytical thinking",
      "Ownership mindset",
      "Documentation and process clarity",
      "System design and architecture",
      "Full-stack development mindset"
    ]
  },

  "interview_screening": {
    "screening_questions": {
      "power_bi_experience": {
        "question": "How many years of Power BI Analyst experience do you have?",
        "answer": "I have basic Power BI knowledge from self-learning only, with no professional or production experience. I built simple practice dashboards as part of my self-study to understand core concepts. However, I have 5+ years of professional reporting experience in Oracle Applications with scheduled concurrent programs, complex report development, and BI Publisher reports, which demonstrates my strong foundation in data reporting and analytics."
      },
      "data_analysis_experience": {
        "question": "How many years of Data and Reporting Analyst experience do you have?",
        "answer": "I have approximately 1 year of structured self-learning in data analytics with SQL-based analysis and Excel proficiency. While I haven't held a dedicated BI analyst title, I have 5+ years of professional reporting experience with Oracle Applications (BI Publisher, concurrent programs) and 2.7 years at AWS where my role involved metric-driven troubleshooting, analyzing CloudWatch performance data, and translating technical insights into actionable recommendations for stakeholders."
      },
      "excel_proficiency": {
        "question": "Do you have experience using Microsoft Excel? If yes, at what level?",
        "answer": "Yes, advanced proficiency. I have built multi-sheet analytical workbooks using PivotTables, Power Query for data transformations, advanced formulas (INDEX/MATCH, nested IFs), data validation logic, and structured reporting. Excel has been my primary tool for data analysis and validation throughout my career."
      },
      "office_products": {
        "question": "Which Microsoft Office products are you experienced with?",
        "answer": "Excel (Advanced), Power Query (Advanced), VBA (Basic exposure), and Access (Basic familiarity). I also have extensive experience with Oracle Applications reporting, scheduled concurrent programs, and Oracle reports."
      },
      "crm_experience": {
        "question": "Do you have CRM tool administration or configuration experience?",
        "answer": "I have not been a dedicated administrator for platforms like Salesforce or Dynamics. However, I have worked extensively with CRM-style customer and ticketing datasets, supporting data validation, reporting accuracy, and scheduled metrics delivery. I identified inconsistencies, ensured reliable metrics for stakeholder decisions, and maintained strict data accuracy standards in operational reporting."
      },
      "location_and_availability": {
        "question": "Are you based in Sydney or willing to relocate? What is your notice period?",
        "answer": "I am currently based in Sydney and seeking Sydney-based roles. The hybrid arrangement suits my circumstances perfectly. I am not currently employed and am immediately available to start."
      },
      "salary_expectations": {
        "question": "What is your expected annual base salary?",
        "answer": "$80,000-$90,000 AUD is appropriate for my level of experience and aligns with the role's advertised range."
      },
      "work_authorization": {
        "question": "What is your right to work in Australia?",
        "answer": "[Candidate would select appropriate option during application based on visa/citizenship status]"
      },
      "recent_projects": {
        "question": "What projects have you worked on recently?",
        "answer": "Recently, I've been working on two AI-driven projects as part of an AI Data Analyst internship program with an AWS consulting team, where I focused on building practical, production-oriented solutions. The first project involved building a personal AI assistant for food knowledge, using Retrieval-Augmented Generation (RAG). I initially developed the system locally so it could learn from my own documents while maintaining full privacy. The assistant can ingest custom knowledge and provide context-aware responses, essentially acting as a private AI brain. From a technical perspective, I used the Llama 3.2 model for language generation and MSBAI embeddings for semantic text representation. The pipeline converts documents into embeddings, stores them in a vector database, and retrieves relevant context before generating responses. I ran the system locally using Ollama and implemented a full RAG pipeline including data preparation, embedding generation, vector storage, and contextual response generation. In the next phase, I migrated the solution to a cloud-native architecture to improve scalability, speed, and accessibility. I replaced the local ChromaDB vector store with Upstash Vector, a serverless vector database with semantic search capabilities, and upgraded the inference engine from Ollama to Groq-powered LLMs for ultra-fast response times and low latency. This involved redesigning the architecture, updating database clients, validating retrieval accuracy, and reconfiguring model integrations. To make the solution accessible through a browser, I am currently wrapping the backend pipeline into a web application. I am converting the Python-based workflow into a Next.js web app, using v0.dev to generate a production-ready frontend interface. In parallel, I am developing a second project: an AI-powered Digital Twin of myself designed to assist with job interview preparation and career development. This system uses RAG and NLP techniques to retrieve verified information from my career history and generate accurate, context-aware interview responses. The AI agent can ingest a job description, simulate interview questions, and generate a comprehensive evaluation report. The first phase runs inside VS Code Agent Mode for full interview simulation, while the second phase includes a web interface for structured practice sessions. My next phase is to enable a complete, interactive interview simulation directly within the web UI. These projects have strengthened my skills in AI system design, RAG pipelines, vector databases, cloud deployment, and building practical tools that solve real-world problems."
      }
    }
  },

  "interview_prep": {
    "response_strategies": {
      "handling_skill_gaps": {
        "when_to_use": "When asked about tools, technologies, or experiences not in your background",
        "approach": "Acknowledge the gap honestly, then pivot to demonstrate learning agility and growth mindset",
        "template_response": "I don't currently have direct experience with [specific skill/tool/technology]. However, as you can see from my past learning experiences and career transitions, I've always found it challenging and enthusiastic to shift my career direction and upskill myself. For example, I successfully transitioned from Oracle Applications to AWS cloud support, and more recently completed an AI Data Analyst internship with AusBiz Consulting where I worked on structured data analytics projects including Python, RAG systems, and vector databases. Gaining this additional skill or getting to work on this new tool would be definitely challenging as well as encouraging to me. I'm confident in my ability to learn quickly and apply new knowledge effectively.",
        "key_points_to_emphasize": [
          "Honest acknowledgment - builds trust and credibility",
          "Evidence of learning agility - cite specific examples of past transitions",
          "Growth mindset - express genuine enthusiasm for learning",
          "Track record - reference successful self-learning projects",
          "Confidence in transferable skills - show how you approach learning"
        ],
        "examples": {
          "tableau": "I don't currently have Tableau experience, though I have basic Power BI knowledge and understand data visualization principles. As you can see from my background, I successfully transitioned from Oracle Applications to AWS cloud support, and more recently completed an AI Data Analyst internship with AusBiz Consulting where I worked on data analysis and visualization projects. Learning Tableau would leverage my analytical foundation while expanding my toolset. I'm enthusiastic about the challenge and confident I can become productive quickly.",
          "machine_learning": "I don't have formal machine learning experience, but I've worked with RAG systems, vector databases, and LLM integration during my AI Data Analyst internship with AusBiz Consulting where I built a Digital Twin project. As demonstrated by my career transitions and practical project experience, I find it challenging and encouraging to expand into new technical areas. Gaining ML skills would complement my data analysis background and align perfectly with my interest in AI-driven analytics.",
          "salesforce": "I haven't administered Salesforce directly, though I have extensive experience with CRM-style datasets, data validation, and reporting accuracy. My 5+ years with Oracle Applications and recent AWS support work show my ability to quickly learn enterprise platforms. I'm genuinely enthusiastic about expanding into Salesforce administration and confident my ERP background provides a strong foundation."
        }
      }
    },
    "behavioral": [
      {
        "question": "How do you differentiate between customer misconfiguration and an AWS service issue?",
        "answer_star": {
          "situation": "Customers often reported errors without clarity on root cause.",
          "task": "Determine whether the issue originated from AWS services or customer configuration.",
          "action": [
            "Reproduced the issue in a lab environment",
            "Matched customer configuration as closely as possible",
            "Validated behavior against expected service outcomes"
          ],
          "result": "Accurate root cause identification and confident escalation or resolution guidance."
        }
      },
      {
        "question": "Describe a time you enabled a non-technical customer.",
        "answer_star": {
          "situation": "Customers lacked visibility into database performance and error causes.",
          "task": "Enable customers to independently monitor and understand RDS health.",
          "action": [
            "Used screen sharing to navigate CloudWatch dashboards",
            "Explained instance class limits and read/write capacity",
            "Shared step-by-step documentation and internal knowledge articles"
          ],
          "result": "Customers gained confidence in interpreting metrics and reduced repeat support queries."
        }
      },
      {
        "question": "Tell me about a time you went beyond your role.",
        "answer_star": {
          "situation": "AWS support scope excludes coding assistance.",
          "task": "Unblock customers without violating role boundaries.",
          "action": [
            "Used prior SQL and PL/SQL experience",
            "Guided customers through query logic rather than writing full solutions",
            "Explained best practices conceptually"
          ],
          "result": "Customers resolved blockers while support boundaries were maintained."
        }
      },
      {
        "question": "Describe a time you handled a high-severity incident.",
        "answer_star": {
          "situation": "A critical production outage impacted multiple customers and degraded service availability.",
          "task": "Coordinate an immediate response to restore service and communicate clearly with stakeholders.",
          "action": [
            "Assembled a cross-functional incident response team and established a war room",
            "Reproduced the issue in a controlled environment to isolate the fault",
            "Implemented a rollback and temporary mitigations while engineering developed a permanent fix",
            "Provided regular, transparent updates to affected customers and internal stakeholders"
          ],
          "result": "Service was restored within the SLA window, customer impact was minimized, and post-incident actions reduced recurrence risk."
        }
      },
      {
        "question": "Give an example of influencing product improvements from customer feedback.",
        "answer_star": {
          "situation": "Multiple customers reported recurring pain points caused by a missing platform feature.",
          "task": "Collect evidence and advocate for a product change to address the root cause.",
          "action": [
            "Collated customer cases and reproducible steps to demonstrate the issue",
            "Filed a detailed internal bug/feature request with impact analysis",
            "Collaborated with product and engineering teams to prioritize and scope a fix",
            "Worked with documentation to ensure customers received guidance during rollout"
          ],
          "result": "The feature was implemented in a subsequent release, lowering support ticket volume and improving customer satisfaction."
        }
      },
      {
        "question": "Tell me about a time you prioritized data quality and ensured accuracy in reporting.",
        "answer_star": {
          "situation": "Monthly stakeholder metrics showed unexpected spikes, raising concerns about data integrity.",
          "task": "Identify and correct data quality issues to restore stakeholder confidence.",
          "action": [
            "Analyzed underlying data and identified inflated metrics caused by duplicate records in source data",
            "Discovered incorrect aggregation logic in the previous calculation structure",
            "Corrected the data model and removed duplication at the source level",
            "Revalidated all totals against source systems before republishing dashboard",
            "Implemented validation checks to prevent future occurrences"
          ],
          "result": "Dashboard metrics now align with source systems. Stakeholders regained confidence in reporting accuracy and made decisions backed by reliable data."
        }
      },
      {
        "question": "Describe your approach to explaining complex data to non-technical stakeholders.",
        "answer_star": {
          "situation": "During critical production outages, needed to present technical performance data and root cause analysis to stakeholder audiences in WAR room (mix of technical and non-technical participants).",
          "task": "Communicate complex technical data, performance metrics, and incident trends in an understandable, actionable format under time pressure.",
          "action": [
            "Focused on business impact and trends rather than technical calculations or methodology",
            "Used visuals (charts, comparisons, trend lines) to make patterns immediately clear",
            "Explained what numbers mean for operations and performance, not how formulas were built",
            "Structured presentation around business questions rather than data structure",
            "Connected insights to specific decisions stakeholders needed to make"
          ],
          "result": "Stakeholders understood the insights, engaged with analysis, and used findings to prioritize operational improvements. Ad hoc analysis requests increased due to confidence in clarity."
        }
      },
      {
        "question": "Tell me about a time you used data analysis to discover an unexpected insight that led to action.",
        "answer_star": {
          "situation": "Customer reported sudden, recurring downtime of their production RDS instance causing business disruption. Customer wanted to understand the root cause.",
          "task": "Analyze system data to identify why the RDS instance was failing over and provide actionable recommendations.",
          "action": [
            "Reviewed instance events logs and identified failover pattern occurring at the same time every week",
            "Queried Performance Insights and Enhanced Monitoring metrics to correlate timing with resource utilization",
            "Analyzed write IOPS patterns and discovered massive spikes exceeding instance capacity during failover windows",
            "Cross-referenced spike timing with customer's operational schedule",
            "Discussed findings with customer to confirm scheduled operations during that time window",
            "Discovered customer had a scheduled concurrent program running weekly that they were unaware was causing the issue"
          ],
          "result": "Customer identified the previously unknown scheduled program as root cause. They rescheduled the job to off-peak hours and increased instance IOPS capacity. Failovers stopped completely, restoring system reliability and preventing future business disruptions. This demonstrated the value of pattern recognition and metric-driven troubleshooting in preventing recurring incidents.",
          "analytical_approach": {
            "pattern_recognition": "Noticed temporal consistency (same time weekly) rather than treating as random events",
            "data_correlation": "Combined event logs, Performance Insights, and Enhanced Monitoring to build complete picture",
            "hypothesis_testing": "Used data to validate suspected resource exhaustion as failure mechanism",
            "stakeholder_collaboration": "Engaged customer with data-backed findings to uncover business context"
          },
          "skills_demonstrated": [
            "SQL querying for log analysis and metric retrieval",
            "Time-series data analysis and pattern detection",
            "Performance metrics interpretation (IOPS, CPU, memory)",
            "Root cause analysis methodology",
            "Data-driven communication with non-technical stakeholders",
            "Translating technical metrics into business impact"
          ]
        }
      }
    ],

    "technical": [
      {
        "question": "How does your cloud experience support your transition into data analytics?",
        "answer": {
          "points": [
            "Daily interpretation of CloudWatch metrics and trends",
            "Correlation of performance data to customer impact",
            "Experience explaining numerical insights to non-technical stakeholders",
            "Hands-on data cleaning and visualization during certification projects"
          ]
        }
      },
      {
        "question": "Walk me through your approach to building analytical reports and dashboards.",
        "answer": {
          "approach": "Structured methodology with quality gates - developed through Oracle reporting and self-learning",
          "steps": [
            "1. Requirements - gather business metrics, audience needs, refresh cadence",
            "2. Data sources - identify source systems, data quality, transformation requirements",
            "3. Data modeling - understand relationships and data structure",
            "4. Development - create calculated measures and metrics (Oracle BI Publisher formulas, SQL aggregations)",
            "5. Design - layout for clarity, use appropriate visualizations for insights",
            "6. Validation - reconcile against source systems, test accuracy",
            "7. Stakeholder review - confirm metrics align with business definitions",
            "8. Deploy - document data lineage and calculation logic"
          ],
          "experience": "5+ years developing Oracle BI Publisher reports and scheduled concurrent programs with complex business logic, SQL queries, and parameter-driven reporting. Recently expanded knowledge through self-learning in modern BI tools."
        }
      },
      {
        "question": "How do you ensure data accuracy from source to report?",
        "answer": {
          "approach": "Multi-step reconciliation and validation",
          "process": [
            "Source-to-report reconciliation - validate report totals match source counts exactly",
            "Clear metric definitions - standardize calculation (e.g., 'revenue = net sales ex returns')",
            "Validation checks - automated checks in Power Query or SQL to catch anomalies",
            "Structured data cleaning - document ETL steps, version control transformations",
            "Anomaly detection - flag unusual patterns (10x increase, zero values, duplicates)",
            "Audit trail - maintain version history of dashboard changes and corrections"
          ]
        }
      },
      {
        "question": "What experience do you have with scheduled reports and automated refreshes?",
        "answer": {
          "professional_experience": "5+ years of professional experience with Oracle BI Publisher reports and concurrent programs - scheduling automated reports, managing report parameters, ensuring reliable delivery for operational and leadership stakeholders",
          "technical_knowledge": "Understand modern BI scheduling concepts including data refresh cadence, gateway connectivity, and automated distribution (conceptual knowledge from self-learning)",
          "business_understanding": "Deep understanding of the criticality of reliable scheduling for time-sensitive operational and leadership reporting from years of production support"
        }
      }
    ]
  },

  "career_transition": {
    "from": "Cloud Support Engineer (last role: Aug 2022 - Mar 2025)",
    "to": "Data Analyst / Junior BI Analyst",
    "current_employment_status": "NOT CURRENTLY EMPLOYED - Unemployed since March 2025, actively seeking opportunities",
    "transition_timeline": "1 year of structured learning and analytics projects including AI Data Analyst internship with AusBiz Consulting (Mar 2025 - present)",
    "evidence_of_transition": [
      "Currently undergoing AI Data Analyst internship with AusBiz Consulting (Dec 2025 - present)",
      "Built production-level AI/ML projects including RAG systems and vector database integration",
      "Basic Power BI knowledge from self-learning (no professional experience)",
      "Advanced Excel proficiency with Power Query, PivotTables, advanced formulas",
      "SQL-based data analysis and querying skills",
      "Data quality ownership and source-to-report reconciliation discipline",
      "Formal analytics certifications (CompTIA Data+, Azure Data Fundamentals)",
      "Metric-driven troubleshooting and pattern recognition from cloud support",
      "Strong numerical reasoning and data interpretation skills",
      "Experience presenting complex data to stakeholder audiences in WAR room during critical production issues",
      "Structured self-learning with applied projects and dashboard validation",
      "5+ years of oracle reports and BI Publisher reports, concurrent programs, migrations, interface and conversions background in Oracle Applications"
    ],
    "transferable_skills": {
      "customer_communication": "Evolved from general support to data storytelling and insight presentation",
      "troubleshooting_methodology": "Applied to data anomaly detection and root cause analysis in quality issues",
      "documentation": "Supports metric definitions, ETL documentation, and data lineage clarity",
      "ownership_mindset": "Applied to data quality responsibility and validation discipline",
      "reporting_experience": "5+ years with Oracle translates to understanding business reporting requirements"
    },
    "readiness_assessment": "READY FOR Junior BI/Data Analyst and AI-focused data roles - 5+ years Oracle BI Publisher and reporting experience, advanced Excel and SQL skills, AWS metrics analysis background. Currently undergoing AI Data Analyst internship with AusBiz Consulting (Dec 2025 - present) working on RAG systems, vector databases, and Python-based data pipelines. Basic modern BI tool knowledge (Power BI) from self-learning. Well-positioned for roles emphasizing SQL, Excel, data analysis, and emerging AI/ML technologies with opportunity to develop modern BI tool proficiency on the job. NOT currently employed in this capacity."
  }
}
